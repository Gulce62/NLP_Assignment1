# NLP_Assignment1

This project explores the fundamental laws of statistical natural language processing and focuses on the development of skills to devise language data, form a corpus, and perform basic pre-processing on the data textual data from the Gutenberg Project website. The collected textual data is analyzed using several techniques, including Zipf’s Law, Heaps’ Law, and clustering methodologies. The results of the assignment indicate that all corpora conform to Zipf’s Law, and the relationship between increasing vocabulary size and the number of tokens is consistent with Heaps’ Law. The findings also suggest that books of the same authors and type have similar slopes for the best fit, enabling the clustering of corpora based on these parameters. The impact of stop words on the corpus was not as expected, and creating a random text produced different results than an actual text. 
